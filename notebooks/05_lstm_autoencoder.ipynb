{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Autoencoder\n",
    "\n",
    "2026-01-28\n",
    "\n",
    "Reconstruction-based LSTM: encodes sequence, reconstructs it. High reconstruction error = anomaly.\n",
    "Comparing full-size (100 units, SKAB config) vs tiny (16 units) for edge deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../data/raw/SKAB/data')\n",
    "\n",
    "files = []\n",
    "for folder in ['valve1', 'valve2', 'other']:\n",
    "    files.extend(sorted((data_dir / folder).glob('*.csv')))\n",
    "\n",
    "feature_cols = ['Accelerometer1RMS', 'Accelerometer2RMS', 'Current', 'Pressure',\n",
    "                'Temperature', 'Thermocouple', 'Voltage', 'Volume Flow RateRMS']\n",
    "\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 400\n",
    "N_STEPS = 10\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "VAL_SPLIT = 0.1\n",
    "Q = 0.99\n",
    "\n",
    "def create_sequences(values, n_steps):\n",
    "    output = []\n",
    "    for i in range(len(values) - n_steps + 1):\n",
    "        output.append(values[i:(i + n_steps)])\n",
    "    return np.stack(output)\n",
    "\n",
    "def build_lstm_ae(n_steps, n_features, units):\n",
    "    inputs = Input(shape=(n_steps, n_features))\n",
    "    encoded = LSTM(units, activation='relu')(inputs)\n",
    "    decoded = RepeatVector(n_steps)(encoded)\n",
    "    decoded = LSTM(units, activation='relu', return_sequences=True)(decoded)\n",
    "    decoded = TimeDistributed(Dense(n_features))(decoded)\n",
    "    model = Model(inputs, decoded)\n",
    "    model.compile(optimizer='adam', loss='mae', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "def evaluate_lstm_ae(files, units, verbose=True):\n",
    "    all_y_true, all_y_pred = [], []\n",
    "    \n",
    "    for i, f in enumerate(files):\n",
    "        df = pd.read_csv(f, sep=';', parse_dates=['datetime'], index_col='datetime')\n",
    "        X, y = df[feature_cols].values, df['anomaly'].values\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_sc = scaler.fit_transform(X[:TRAIN_SIZE])\n",
    "        X_test_sc = scaler.transform(X[TRAIN_SIZE:])\n",
    "        \n",
    "        X_tr = create_sequences(X_train_sc, N_STEPS)\n",
    "        X_te = create_sequences(X_test_sc, N_STEPS)\n",
    "        \n",
    "        tf.random.set_seed(0)\n",
    "        np.random.seed(0)\n",
    "        model = build_lstm_ae(N_STEPS, len(feature_cols), units)\n",
    "        model.fit(X_tr, X_tr, validation_split=VAL_SPLIT, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                  verbose=0, shuffle=False, callbacks=[EarlyStopping(patience=5, verbose=0)])\n",
    "        \n",
    "        train_recon = model.predict(X_tr, verbose=0)\n",
    "        residuals_train = np.sum(np.mean(np.abs(X_tr - train_recon), axis=1), axis=1)\n",
    "        UCL = np.percentile(residuals_train, Q * 100) * 3 / 2\n",
    "        \n",
    "        test_recon = model.predict(X_te, verbose=0)\n",
    "        residuals_test = np.sum(np.mean(np.abs(X_te - test_recon), axis=1), axis=1)\n",
    "        \n",
    "        # SKAB anomaly logic: flag if all samples in window are anomalous\n",
    "        anomalous_data = residuals_test > UCL\n",
    "        anomalous_indices = []\n",
    "        for idx in range(N_STEPS - 1, len(X_te) - N_STEPS + 1):\n",
    "            if np.all(anomalous_data[idx - N_STEPS + 1:idx]):\n",
    "                anomalous_indices.append(idx)\n",
    "        \n",
    "        pred = np.zeros(len(y[TRAIN_SIZE:]))\n",
    "        for idx in anomalous_indices:\n",
    "            if idx < len(pred):\n",
    "                pred[idx] = 1\n",
    "        \n",
    "        all_y_true.extend(y[TRAIN_SIZE:])\n",
    "        all_y_pred.extend(pred)\n",
    "        \n",
    "        if verbose and (i+1) % 10 == 0:\n",
    "            print(f'{i+1}/{len(files)} files')\n",
    "    \n",
    "    y_true, y_pred = np.array(all_y_true), np.array(all_y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    return {'f1': f1, 'far': fp/(fp+tn)*100, 'mar': fn/(fn+tp)*100,\n",
    "            'size_kb': len(pickle.dumps(model))/1024, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full-size (100 units)\n",
    "results_full = evaluate_lstm_ae(files, units=100)\n",
    "print(f\"F1: {results_full['f1']:.2f} (SKAB: 0.74), Size: {results_full['size_kb']:.0f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiny (16 units)\n",
    "results_tiny = evaluate_lstm_ae(files, units=16)\n",
    "print(f\"F1: {results_tiny['f1']:.2f} (full: {results_full['f1']:.2f}), Size: {results_tiny['size_kb']:.0f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Full (100 units)', 'Tiny (16 units)'],\n",
    "    'F1': [results_full['f1'], results_tiny['f1']],\n",
    "    'FAR %': [results_full['far'], results_tiny['far']],\n",
    "    'MAR %': [results_full['mar'], results_tiny['mar']],\n",
    "    'Size KB': [results_full['size_kb'], results_tiny['size_kb']]\n",
    "})\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_drop = (results_full['f1'] - results_tiny['f1']) / results_full['f1'] * 100\n",
    "size_reduction = (results_full['size_kb'] - results_tiny['size_kb']) / results_full['size_kb'] * 100\n",
    "print(f\"F1 drop: {f1_drop:.1f}%, Size reduction: {size_reduction:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latency\n",
    "sample = pd.read_csv(files[5], sep=';', parse_dates=['datetime'], index_col='datetime')\n",
    "sc = StandardScaler()\n",
    "X_sc = sc.fit_transform(sample[feature_cols].values[:TRAIN_SIZE])\n",
    "X_seq = create_sequences(X_sc, N_STEPS)\n",
    "single = X_seq[:1]\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "m_full = build_lstm_ae(N_STEPS, 8, 100)\n",
    "m_full.fit(X_seq, X_seq, epochs=3, verbose=0)\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "m_tiny = build_lstm_ae(N_STEPS, 8, 16)\n",
    "m_tiny.fit(X_seq, X_seq, epochs=3, verbose=0)\n",
    "\n",
    "m_full.predict(single, verbose=0)\n",
    "m_tiny.predict(single, verbose=0)\n",
    "\n",
    "times = []\n",
    "for _ in range(500):\n",
    "    t0 = time.perf_counter()\n",
    "    m_full.predict(single, verbose=0)\n",
    "    times.append(time.perf_counter() - t0)\n",
    "lat_full = np.median(times) * 1000\n",
    "\n",
    "times = []\n",
    "for _ in range(500):\n",
    "    t0 = time.perf_counter()\n",
    "    m_tiny.predict(single, verbose=0)\n",
    "    times.append(time.perf_counter() - t0)\n",
    "lat_tiny = np.median(times) * 1000\n",
    "\n",
    "print(f\"Full: {lat_full:.1f} ms, Tiny: {lat_tiny:.1f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "LSTM-AE more resilient to shrinking than Vanilla LSTM: ~94% size reduction with only ~3% F1 drop.\n",
    "Best accuracy of all models tested (F1â‰ˆ0.75), and tiny version still outperforms other methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
